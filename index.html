<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Constructing Secure AI Agent Systems</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Constructing Secure AI Agent Systems</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <!--               <span class="author-block">
                                        <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                                        <span class="author-block">
                                          <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                                          <span class="author-block">
                                            <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                                          </span> -->
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">The Hong Kong Polytechnic University (PolyU)</span>
                        <!--                     <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <!--                       <span class="link-block">
                                                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                                    class="external-link button is-normal is-rounded is-dark">
                                                    <span class="icon">
                                                      <i class="fas fa-file-pdf"></i>
                                                    </span>
                                                    <span>Paper</span>
                                                  </a>
                                                </span> -->

                            <!-- Supplementary PDF link -->
                            <!--                     <span class="link-block">
                                                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                                  class="external-link button is-normal is-rounded is-dark">
                                                  <span class="icon">
                                                    <i class="fas fa-file-pdf"></i>
                                                  </span>
                                                  <span>Supplementary</span>
                                                </a>
                                              </span> -->

                            <!-- Github link -->
                            <!--                   <span class="link-block">
                                                <a href="https://github.com/YOUR REPO HERE" target="_blank"
                                                class="external-link button is-normal is-rounded is-dark">
                                                <span class="icon">
                                                  <i class="fab fa-github"></i>
                                                </span>
                                                <span>Code</span>
                                              </a>
                                            </span> -->

                            <!-- ArXiv abstract Link -->
                            <!--                 <span class="link-block">
                                              <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                              class="external-link button is-normal is-rounded is-dark">
                                              <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                              </span>
                                              <span>arXiv</span>
                                            </a>
                                          </span> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/agenthistory.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 1. Major events related to AI agents and its security.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/aiframework.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 2. Current AI agent systems suffer from various adversarial attacks and generate harmful
                        or illegal responses.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/secureAgent.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 3. An overview of the proposed secure AI agent system framework.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Embracing Secure AI Agents</h2>
                <div class="content has-text-justified">
                    <p>
                        AI agents are emerging as the most promising pathway for deploying large language models (LLMs)
                        in real-world applications. Advancements in LLMs have significantly enhanced their ability to
                        process complex user interactions and perform contextual reasoning, enabling AI agents to
                        automate tasks and execute decisions with growing sophistication. These capabilities are driving
                        transformative innovations across industries such as autonomous driving, intelligent office
                        automation, and financial services.
                    </p>
                    <p>
                        However, as shown in Figure 1, as AI agents become deeply integrated into mission-critical
                        systems, their expanded role introduces novel security risks. Uncontrollable user inputs, opaque
                        decision-making logic, and dynamic operating environments collectively expose AI agents to
                        diverse attack vectors. For instance, malicious actors could exploit vulnerabilities in
                        web-based agents to place fraudulent orders or hijack workflows to execute unauthorized
                        commands. These challenges highlight an urgent need for robust security frameworks tailored to
                        AI agents’ evolving threat landscape, ensuring both functional reliability and safe real-world
                        deployment.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">1.Diffusion Models as Strong Adversaries</h2>
                <div class="content has-text-justified">
                    <p>
                        Diffusion models have demonstrated their great ability to generate high-quality images for
                        various tasks. With such a strong performance, diffusion models can potentially pose a severe
                        threat to both humans and deep learning models, e.g., DNNs and MLLMs. However, their abilities
                        as adversaries have not been well explored. Among different adversarial scenarios, the no-box
                        adversarial attack is the most practical one, as it assumes that the attacker has no access to
                        the training dataset or the target model. Existing works still require some data from the
                        training dataset, which may not be feasible in real-world scenarios. In this paper, we
                        investigate the adversarial capabilities of diffusion models by conducting no-box attacks solely
                        using data generated by diffusion models. Specifically, our attack method generates a synthetic
                        dataset using diffusion models to train a substitute model. We then employ a classification
                        diffusion model to fine-tune the substitute model, considering model uncertainty and
                        incorporating noise augmentation. Finally, we sample adversarial examples from the diffusion
                        models using the average approximation over the diffusion substitute model with multiple
                        inferences. Extensive experiments on the ImageNet dataset demonstrate that the proposed attack
                        method achieves state-of-the-art performance in both no-box attack and black-box attack
                        scenarios.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/no-box_adversarial_attacks.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 4. The attack pipeline of our proposed no-box adversarial attacks.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/Diffusion_Models_as_Strong_Adversaries_4.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 5. FComparisons of no-box adversarial examples with our method and Li et al.’s method..
                        Note that our method achieves a similar ASR with a significantly lower perturbation. The adversarial examples are generated by latent inversion.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@article{dai2024diffusion,
  title={Diffusion Models as Strong Adversaries},
  author={Dai, Xuelong and Li, Yanjie and Duan, Mingxing and Xiao, Bin},
  journal={IEEE Transactions on Image Processing},
  year={2024},
  publisher={IEEE}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">2. Advdiff: Generating unrestricted adversarial examples using diffusion models</h2>
                <div class="content has-text-justified">
                    <p>
                        Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often directly inject Projected Gradient Descent (PGD) gradients into the sampling of generative models, which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for GAN-based methods on large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable in generating high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective in generating unrestricted adversarial examples, which outperforms state-of-the-art unrestricted adversarial attack methods in terms of attack performance and generation quality.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/AdvDiff_1.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 6. The two new guidance techniques in our AdvDiff to generate unrestricted adversarial examples.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/AdvDiff_2.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 7. Comparisons of unrestricted adversarial attacks between GANs and diffusion models on two datasets.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@inproceedings{dai2024advdiff,
  title={Advdiff: Generating unrestricted adversarial examples using diffusion models},
  author={Dai, Xuelong and Liang, Kaisheng and Xiao, Bin},
  booktitle={European Conference on Computer Vision},
  pages={93--109},
  year={2024},
  organization={Springer}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">3. Styless: Boosting the transferability of adversarial examples</h2>
                <div class="content has-text-justified">
                    <p>
                        Adversarial attacks can mislead deep neural networks (DNNs) by adding imperceptible perturbations to benign examples. The attack transferability enables adversarial examples to attack black-box DNNs with unknown architectures or parameters, which poses threats to many real-world applications. We find that existing transferable attacks do not distinguish between style and content features during optimization, limiting their attack transferability. To improve attack transferability, we propose a novel attack method called style-less perturbation (StyLess). Specifically, instead of using a vanilla network as the surrogate model, we advocate using stylized networks, which encode different style features by perturbing an adaptive instance normalization. Our method can prevent adversarial examples from using non-robust style features and help generate transferable perturbations. Comprehensive experiments show that our method can significantly improve the transferability of adversarial examples. Furthermore, our approach is generic and can outperform state-of-the-art transferable attacks when combined with other attack techniques.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<!--TODO: Code and Picture-->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/AdvDiff_1.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 6. The two new guidance techniques in our AdvDiff to generate unrestricted adversarial examples.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/AdvDiff_2.png" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Figure 7. Comparisons of unrestricted adversarial attacks between GANs and diffusion models on two datasets.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@InProceedings{Liang_2023_CVPR,
    author    = {Liang, Kaisheng and Xiao, Bin},
    title     = {StyLess: Boosting the Transferability of Adversarial Examples},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {8163-8172}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">AI Agent Preliminary Demonstration</h2>
                <div class="hero-body">
                    <video poster="" id="tree" autoplay controls muted loop height="100%">
                        <!-- Your video here -->
                        <source src="static/videos/CogAgent1080_3.mp4"
                                type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-centered">
                        Video 1: AI Agent without attack
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Teaser video-->

<!-- End teaser video -->

<!-- Youtube video -->

<!-- End youtube video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Objectives</h2>
                <div class="content has-text-justified">
                    <p>
                        To construct a practical and secure AI agent system that ensures robust and safe AI agent
                        deployment for commercialization. Our unified, secure AI agent system incorporates our proposed
                        safety-aligned LLM models and secure memory access, and provides legitimate AI agent responses.
                        We plan to implement this system across various open-source AI models, thereby enhancing its
                        security and robustness to be applied in the real world.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Video carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-3">Video Presentation</h2>
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-video1">
                    <video poster="" id="video1" autoplay controls muted loop height="100%">
                        <!-- Your video file here -->
                        <source src="static/videos/CogAgent1080.mp4"
                                type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-centered">
                        Video 1: AI Agent without attack
                    </h2>
                </div>
                <div class="item item-video2">
                    <video poster="" id="video2" autoplay controls muted loop height="100%">
                        <!-- Your video file here -->
                        <source src="static/videos/CogAgent1080_2.mp4"
                                type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-centered">
                        Video 2: AI Agent with attack
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End video carousel -->


<!-- Paper poster -->
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title">HK RIF proposal (Waiting for upload)</h2>

            <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
            </iframe>

        </div>
    </div>
</section>
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
@article{dai2024diffusion,
  title={Diffusion Models as Strong Adversaries},
  author={Dai, Xuelong and Li, Yanjie and Duan, Mingxing and Xiao, Bin},
  journal={IEEE Transactions on Image Processing},
  year={2024},
  publisher={IEEE}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Constructing
                        Secure AI Agent Systems Template</a> which was adopted from the <a
                            href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                        You are free to borrow the source code of this website, we just ask that you link back to this
                        page in the footer. <br> This website is licensed under a <a rel="license"
                                                                                     href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                                     target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
